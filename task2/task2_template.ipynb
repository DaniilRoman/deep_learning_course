{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 classification with a neural network\n",
    "# Author: Daniil Roman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load CIFAR-10 dataset and prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_downloader import cifar10\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_data(channel_numbers=3, img_dim=32, train_size=None, test_size=None):\n",
    "    # already normalised\n",
    "    train_x, train_y, test_x, test_y = cifar10()\n",
    "    train_y = np.array([np.argmax(i) for i in train_y]).reshape(len(train_x), 1)\n",
    "    train_data = np.hstack((train_x, train_y))\n",
    "\n",
    "    test_y = np.array([np.argmax(i) for i in test_y]).reshape(len(test_y), 1)\n",
    "    test_x = test_x.reshape(len(test_x), channel_numbers, img_dim, img_dim)\n",
    "\n",
    "    if (train_size != None):\n",
    "        return train_data[:train_size], test_x[:test_size], test_y[:test_size].flatten()\n",
    "    return train_data, test_x, test_y.flatten()\n",
    "\n",
    "train_size = 10\n",
    "test_size = 5\n",
    "img_dim = 32\n",
    "channel_numbers = 3\n",
    "train_data, test_x, test_y = prepare_data(channel_numbers, img_dim, train_size=train_size, test_size=test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NN implementation, layers, activations, loss functions and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UTILS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_x_y(data, n_c=3, dim=32):\n",
    "    X = data[:, 0:-1]\n",
    "    X = X.reshape(len(data), n_c, dim, dim)\n",
    "    Y = data[:, -1:]\n",
    "    return X, Y.flatten()\n",
    "\n",
    "def xavier_init(size):\n",
    "    bound = np.sqrt(2. / np.sum(size))\n",
    "    return np.random.uniform(-bound, bound, size=size)\n",
    "\n",
    "\n",
    "def softmax_crossentropy(forward_output, y):\n",
    "    y = y.flatten().astype(int)\n",
    "    forward_output_for_answers = forward_output[np.arange(len(forward_output)), y]\n",
    "    return - forward_output_for_answers + np.log(np.sum(np.exp(forward_output), axis=-1))\n",
    "\n",
    "\n",
    "def grad_softmax_crossentropy(forward_output, y):\n",
    "    ones_for_answers = np.zeros_like(forward_output)\n",
    "    ones_for_answers[np.arange(len(forward_output)), y.flatten().astype(int)] = 1\n",
    "\n",
    "    softmax = np.exp(forward_output) / np.exp(forward_output).sum(axis=-1, keepdims=True)\n",
    "\n",
    "    return (- ones_for_answers + softmax) / forward_output.shape[0]\n",
    "\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "\n",
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_height) % stride == 0\n",
    "    out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "    out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n",
    "                   stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MODEL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Step:\n",
    "    def __init__(self):\n",
    "        self.forward_input = None\n",
    "        self.backward_param = None\n",
    "\n",
    "        self.learning_rate = None\n",
    "\n",
    "    def forward(self, image):\n",
    "        output = self._forward(image)\n",
    "        self.forward_input = image\n",
    "        return output\n",
    "\n",
    "    def _forward(self, image):\n",
    "        raise Exception(\"Not implemented.\")\n",
    "\n",
    "    def backward(self, input, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        backward_param = self._backward(input)\n",
    "        self.backward_param = backward_param\n",
    "        return backward_param\n",
    "\n",
    "    def _backward(self, input):\n",
    "        raise Exception(\"Not implemented.\")\n",
    "\n",
    "    def get_derivatives(self):\n",
    "        return []\n",
    "\n",
    "    def init_derivative(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, lr):\n",
    "        self.steps = []\n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def forward(self, input):\n",
    "        current_step_output = input\n",
    "        for step in self.steps:\n",
    "            current_step_output = step.forward(current_step_output)\n",
    "        return current_step_output\n",
    "\n",
    "    def backward(self, derivative_of_loss):\n",
    "        current_step_output = derivative_of_loss\n",
    "        self.steps.reverse()\n",
    "        for step in self.steps:\n",
    "            current_step_output = step.backward(current_step_output, self.learning_rate)\n",
    "        self.steps.reverse()\n",
    "\n",
    "    def get_derivatives(self):\n",
    "        grads = []\n",
    "\n",
    "        for step in self.steps:\n",
    "            grads += step.get_derivatives()\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def init_derivative(self):\n",
    "        for step in self.steps:\n",
    "            step.init_derivative()\n",
    "\n",
    "    def predict(self, x):\n",
    "        result = self.forward(x)\n",
    "        return result.argmax(axis=-1)\n",
    "\n",
    "\n",
    "class Convolution(Step):\n",
    "\n",
    "    def __init__(self, size, stride=1, padding=0):\n",
    "        super(Convolution, self).__init__()\n",
    "        filter, bios = self._init(size)\n",
    "\n",
    "        self.filter, self.bios, self.stride, self.padding = filter, bios, stride, padding\n",
    "        self.init_derivative()\n",
    "\n",
    "    def init_derivative(self):\n",
    "        self.derivative_filter, self.derivative_bios = np.zeros(self.filter.shape), np.zeros(self.bios.shape)\n",
    "\n",
    "    def _init(self, size):\n",
    "        filter = xavier_init(size)\n",
    "        bios = np.zeros((filter.shape[0], 1))\n",
    "        return filter, bios\n",
    "\n",
    "    def _forward(self, X):\n",
    "        n_filters, d_filter, h_filter, w_filter = self.filter.shape\n",
    "        n_x, d_x, h_x, w_x = X.shape\n",
    "        h_out = (h_x - h_filter + 2 * self.padding) / self.stride + 1\n",
    "        w_out = (w_x - w_filter + 2 * self.padding) / self.stride + 1\n",
    "\n",
    "        if not h_out.is_integer() or not w_out.is_integer():\n",
    "            raise Exception('Invalid output dimension!')\n",
    "\n",
    "        h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "        X_col = im2col_indices(X, h_filter, w_filter, padding=self.padding, stride=self.stride)\n",
    "        W_col = self.filter.reshape(n_filters, -1)\n",
    "\n",
    "        out = W_col @ X_col + self.bios\n",
    "        out = out.reshape(n_filters, h_out, w_out, n_x)\n",
    "        out = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "        self.cache = (X, self.filter, self.bios, self.stride, self.padding, X_col)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _backward(self, input):\n",
    "        derivative_conv, derivative_filter, derivative_bios = self.conv_backward(input)\n",
    "        self.derivative_filter += derivative_filter\n",
    "        self.derivative_bios += derivative_bios\n",
    "        self.learning_step(self.learning_rate)\n",
    "        return derivative_conv\n",
    "\n",
    "    def conv_backward(self, input):\n",
    "        X, W, b, stride, padding, X_col = self.cache\n",
    "        n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "        db = np.sum(input, axis=(0, 2, 3))\n",
    "        db = db.reshape(n_filter, -1)\n",
    "\n",
    "        dout_reshaped = input.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "        dW = dout_reshaped @ X_col.T\n",
    "        dW = dW.reshape(W.shape)\n",
    "\n",
    "        W_reshape = W.reshape(n_filter, -1)\n",
    "        dX_col = W_reshape.T @ dout_reshaped\n",
    "        dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def get_derivatives(self):\n",
    "        return [self.derivative_filter, self.derivative_bios]\n",
    "\n",
    "    def learning_step(self, learning_rate):\n",
    "        self.filter -= learning_rate * self.derivative_filter\n",
    "        self.bios -= learning_rate * self.derivative_bios\n",
    "\n",
    "\n",
    "class FullyConnected(Step):\n",
    "\n",
    "    def __init__(self, input_units, output_units):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.weights = xavier_init((input_units, output_units))\n",
    "        self.biases = np.zeros(output_units)\n",
    "\n",
    "    def _forward(self, input):\n",
    "        return input @ self.weights + self.biases\n",
    "\n",
    "    def _backward(self, input):\n",
    "        grad_input = input @ self.weights.T\n",
    "\n",
    "        grad_weights = self.forward_input.T @ input\n",
    "        grad_biases = np.sum(input, axis=0)\n",
    "\n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class Relu(Step):\n",
    "    def _forward(self, input):\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def _backward(self, input):\n",
    "        relu_derivative = self.forward_input > 0\n",
    "        return input * relu_derivative\n",
    "\n",
    "\n",
    "class MaxPool(Step):\n",
    "    def __init__(self, size, stride, padding=0):\n",
    "        super(MaxPool, self).__init__()\n",
    "        self.size, self.stride, self.padding = size, stride, padding\n",
    "\n",
    "    def _forward(self, X):\n",
    "        n, d, h, w = X.shape\n",
    "        h_out = (h - self.size) / self.stride + 1\n",
    "        w_out = (w - self.size) / self.stride + 1\n",
    "\n",
    "        if not w_out.is_integer() or not h_out.is_integer():\n",
    "            raise Exception('Invalid output dimension!')\n",
    "\n",
    "        h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "        X_reshaped = X.reshape(n * d, 1, h, w)\n",
    "        X_col = im2col_indices(X_reshaped, self.size, self.size, padding=self.padding, stride=self.stride)\n",
    "\n",
    "        max_index = np.argmax(X_col, axis=0)\n",
    "        out = X_col[max_index, range(max_index.size)]\n",
    "\n",
    "        out = out.reshape(h_out, w_out, n, d)\n",
    "        out = out.transpose(2, 3, 0, 1)\n",
    "\n",
    "        self.cache = (X, self.size, self.stride, X_col, max_index)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _backward(self, input):\n",
    "        X, size, stride, X_col, pool_cache = self.cache\n",
    "        n, d, w, h = X.shape\n",
    "\n",
    "        dX_col = np.zeros_like(X_col)\n",
    "\n",
    "        dX = col2im_indices(dX_col, (n * d, 1, h, w), size, size, padding=self.padding, stride=stride)\n",
    "        dX = dX.reshape(X.shape)\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Flatten(Step):\n",
    "    def _forward(self, input):\n",
    "        (n_channels, height, width) = input[0].shape\n",
    "        flattened = input.reshape((len(input), n_channels * height * width))\n",
    "        return flattened\n",
    "\n",
    "    def _backward(self, input):\n",
    "        return input.reshape(self.forward_input.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RUN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def SGD(model, X, Y):\n",
    "    out = model.forward(X)\n",
    "\n",
    "    loss = softmax_crossentropy(out, Y)\n",
    "    loss_grad = grad_softmax_crossentropy(out, Y)\n",
    "\n",
    "    model.backward(loss_grad)\n",
    "\n",
    "    return np.mean(loss)\n",
    "\n",
    "def train(train_data,model, img_dim=32, channel_numbers=3, batch_size=4, num_epochs=20, lr=0.01):\n",
    "\n",
    "    print(\"LR:\" + str(lr) + \", Batch Size:\" + str(batch_size))\n",
    "    for epoch in range(num_epochs):\n",
    "        np.random.shuffle(train_data)\n",
    "        batches = [train_data[k:k + batch_size] for k in range(0, train_data.shape[0], batch_size)]\n",
    "\n",
    "        t = tqdm(batches)\n",
    "        for x, batch in enumerate(t):\n",
    "            X, Y = split_to_x_y(batch, channel_numbers, img_dim)\n",
    "            loss = SGD(model, X, Y)\n",
    "            t.set_description(\"Loss: %.2f\" % (loss))\n",
    "\n",
    "def predict(model, test_x, test_y):\n",
    "    result = model.predict(test_x)\n",
    "    assert result.shape == test_y.shape\n",
    "    print(f\"Score: {np.mean(result == test_y)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_data, test_x, test_y = prepare_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "model = Model(lr)\n",
    "\n",
    "model.steps = [\n",
    "    Flatten(),\n",
    "    FullyConnected(3072, 128),\n",
    "    Relu(),\n",
    "    FullyConnected(128, 10)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train with 1 img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.30: 100%|██████████| 1/1 [00:00<00:00, 60.69it/s]\n",
      "Loss: 0.98: 100%|██████████| 1/1 [00:00<00:00, 59.35it/s]\n",
      "Loss: 0.50: 100%|██████████| 1/1 [00:00<00:00, 68.30it/s]\n",
      "Loss: 0.28: 100%|██████████| 1/1 [00:00<00:00, 87.48it/s]\n",
      "Loss: 0.18: 100%|██████████| 1/1 [00:00<00:00, 78.18it/s]\n",
      "Loss: 0.13: 100%|██████████| 1/1 [00:00<00:00, 145.52it/s]\n",
      "Loss: 0.10: 100%|██████████| 1/1 [00:00<00:00, 117.89it/s]\n",
      "Loss: 0.08: 100%|██████████| 1/1 [00:00<00:00, 215.36it/s]\n",
      "Loss: 0.06: 100%|██████████| 1/1 [00:00<00:00, 210.23it/s]\n",
      "Loss: 0.05: 100%|██████████| 1/1 [00:00<00:00, 160.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.01, Batch Size:4\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = train_data[:1]\n",
    "\n",
    "train(train_data_1, model, batch_size=4, num_epochs=10, lr=lr)\n",
    "\n",
    "x, y = split_to_x_y(train_data_1)\n",
    "predict(model, x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train with 10 img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.33: 100%|██████████| 3/3 [00:00<00:00, 72.14it/s]\n",
      "Loss: 2.75: 100%|██████████| 3/3 [00:00<00:00, 108.90it/s]\n",
      "Loss: 1.85: 100%|██████████| 3/3 [00:00<00:00, 191.78it/s]\n",
      "Loss: 1.80: 100%|██████████| 3/3 [00:00<00:00, 145.14it/s]\n",
      "Loss: 2.20: 100%|██████████| 3/3 [00:00<00:00, 227.42it/s]\n",
      "Loss: 1.92: 100%|██████████| 3/3 [00:00<00:00, 138.08it/s]\n",
      "Loss: 1.79: 100%|██████████| 3/3 [00:00<00:00, 226.45it/s]\n",
      "Loss: 1.78: 100%|██████████| 3/3 [00:00<00:00, 104.29it/s]\n",
      "Loss: 2.41: 100%|██████████| 3/3 [00:00<00:00, 103.90it/s]\n",
      "Loss: 1.80: 100%|██████████| 3/3 [00:00<00:00, 202.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.01, Batch Size:4\n",
      "Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "train_data_10 = train_data[:10]\n",
    "\n",
    "train(train_data_10, model, batch_size=4, num_epochs=10, lr=lr)\n",
    "\n",
    "x, y = split_to_x_y(train_data_10)\n",
    "predict(model, x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_T, test_x_T, test_y_T = train_data[:1000], test_x[:100], test_y[:100]\n",
    "lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.29: 100%|██████████| 50/50 [00:01<00:00, 29.20it/s]\n",
      "Loss: 2.31: 100%|██████████| 50/50 [00:01<00:00, 28.95it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:01<00:00, 27.35it/s]\n",
      "Loss: 2.24: 100%|██████████| 50/50 [00:01<00:00, 31.17it/s]\n",
      "Loss: 2.26: 100%|██████████| 50/50 [00:01<00:00, 30.63it/s]\n",
      "Loss: 2.31: 100%|██████████| 50/50 [00:01<00:00, 33.28it/s]\n",
      "Loss: 2.26: 100%|██████████| 50/50 [00:01<00:00, 30.13it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:01<00:00, 28.70it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:01<00:00, 29.15it/s]\n",
      "Loss: 2.21: 100%|██████████| 50/50 [00:01<00:00, 26.21it/s]\n",
      "Loss: 2.14: 100%|██████████| 50/50 [00:01<00:00, 28.74it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:01<00:00, 30.62it/s]\n",
      "Loss: 2.29: 100%|██████████| 50/50 [00:01<00:00, 28.69it/s]\n",
      "Loss: 2.14: 100%|██████████| 50/50 [00:01<00:00, 29.94it/s]\n",
      "Loss: 2.20: 100%|██████████| 50/50 [00:01<00:00, 32.46it/s]\n",
      "Loss: 2.18: 100%|██████████| 50/50 [00:01<00:00, 31.55it/s]\n",
      "Loss: 2.20: 100%|██████████| 50/50 [00:01<00:00, 29.12it/s]\n",
      "Loss: 2.20: 100%|██████████| 50/50 [00:01<00:00, 32.95it/s]\n",
      "Loss: 2.17: 100%|██████████| 50/50 [00:01<00:00, 25.80it/s]\n",
      "Loss: 2.23: 100%|██████████| 50/50 [00:01<00:00, 25.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.001, Batch Size:20\n",
      "Score: 0.24\n"
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "model = Model(lr)\n",
    "\n",
    "model.steps = [\n",
    "    Flatten(),\n",
    "    FullyConnected(3072, 512),\n",
    "    Relu(),\n",
    "    FullyConnected(512, 128),\n",
    "    Relu(),\n",
    "    FullyConnected(128, 10)\n",
    "]\n",
    "\n",
    "train(train_data_T, model, batch_size=20, num_epochs=20, lr=lr)\n",
    "predict(model, test_x_T, test_y_T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.30: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s]\n",
      "Loss: 2.31: 100%|██████████| 50/50 [00:23<00:00,  2.10it/s]\n",
      "Loss: 2.28: 100%|██████████| 50/50 [00:25<00:00,  1.94it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:24<00:00,  2.03it/s]\n",
      "Loss: 2.29: 100%|██████████| 50/50 [00:23<00:00,  2.12it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:27<00:00,  1.80it/s]\n",
      "Loss: 2.31: 100%|██████████| 50/50 [00:28<00:00,  1.75it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:27<00:00,  1.79it/s]\n",
      "Loss: 2.26: 100%|██████████| 50/50 [00:23<00:00,  2.09it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:26<00:00,  1.91it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:26<00:00,  1.89it/s]\n",
      "Loss: 2.29: 100%|██████████| 50/50 [00:27<00:00,  1.82it/s]\n",
      "Loss: 2.28: 100%|██████████| 50/50 [00:25<00:00,  1.96it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:26<00:00,  1.90it/s]\n",
      "Loss: 2.25: 100%|██████████| 50/50 [00:26<00:00,  1.91it/s]\n",
      "Loss: 2.26: 100%|██████████| 50/50 [00:25<00:00,  1.97it/s]\n",
      "Loss: 2.29: 100%|██████████| 50/50 [00:25<00:00,  1.94it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:26<00:00,  1.91it/s]\n",
      "Loss: 2.28: 100%|██████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "Loss: 2.29: 100%|██████████| 50/50 [00:24<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.001, Batch Size:20\n",
      "Score: 0.11\n"
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "model = Model(lr)\n",
    "\n",
    "model.steps = [\n",
    "    Convolution(size=(8, channel_numbers, 5, 5), stride=1),\n",
    "    Relu(),\n",
    "    Convolution(size=(3, 8, 5, 5), stride=1),\n",
    "    Relu(),\n",
    "    MaxPool(size=2, stride=2),\n",
    "    Flatten(),\n",
    "    FullyConnected(432, 256),\n",
    "    Relu(),\n",
    "    FullyConnected(256, 128),\n",
    "    Relu(),\n",
    "    FullyConnected(128, 10)\n",
    "]\n",
    "\n",
    "train(train_data_T, model, batch_size=20, num_epochs=20, lr=lr)\n",
    "predict(model, test_x_T, test_y_T)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.30: 100%|██████████| 50/50 [00:02<00:00, 21.42it/s]\n",
      "Loss: 2.29: 100%|██████████| 50/50 [00:03<00:00, 16.26it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:04<00:00, 11.49it/s]\n",
      "Loss: 2.26: 100%|██████████| 50/50 [00:04<00:00, 11.50it/s]\n",
      "Loss: 2.26: 100%|██████████| 50/50 [00:04<00:00, 11.98it/s]\n",
      "Loss: 2.26: 100%|██████████| 50/50 [00:04<00:00, 11.78it/s]\n",
      "Loss: 2.23: 100%|██████████| 50/50 [00:04<00:00, 11.63it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:04<00:00, 11.30it/s]\n",
      "Loss: 2.24: 100%|██████████| 50/50 [00:04<00:00, 11.80it/s]\n",
      "Loss: 2.36: 100%|██████████| 50/50 [00:04<00:00, 11.31it/s]\n",
      "Loss: 2.22: 100%|██████████| 50/50 [00:04<00:00, 10.86it/s]\n",
      "Loss: 2.09: 100%|██████████| 50/50 [00:04<00:00, 11.02it/s]\n",
      "Loss: 2.24: 100%|██████████| 50/50 [00:04<00:00, 10.66it/s]\n",
      "Loss: 2.28: 100%|██████████| 50/50 [00:04<00:00, 10.86it/s]\n",
      "Loss: 2.19: 100%|██████████| 50/50 [00:04<00:00, 12.40it/s]\n",
      "Loss: 2.29: 100%|██████████| 50/50 [00:04<00:00, 11.00it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:03<00:00, 13.77it/s]\n",
      "Loss: 2.23: 100%|██████████| 50/50 [00:03<00:00, 12.92it/s]\n",
      "Loss: 2.23: 100%|██████████| 50/50 [00:04<00:00, 10.40it/s]\n",
      "Loss: 2.24: 100%|██████████| 50/50 [00:04<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.01, Batch Size:20\n",
      "Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "model = Model(lr)\n",
    "\n",
    "model.steps = [\n",
    "    Convolution(size=(2, channel_numbers, 4, 4), stride=2),\n",
    "    Relu(),\n",
    "    MaxPool(size=5, stride=2),\n",
    "    Flatten(),\n",
    "    FullyConnected(72, 32),\n",
    "    Relu(),\n",
    "    FullyConnected(32, 10)\n",
    "]\n",
    "\n",
    "train(train_data_T, model, batch_size=20, num_epochs=20, lr=lr)\n",
    "predict(model, test_x_T, test_y_T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.31: 100%|██████████| 50/50 [00:01<00:00, 25.03it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:02<00:00, 24.16it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:01<00:00, 25.39it/s]\n",
      "Loss: 2.31: 100%|██████████| 50/50 [00:01<00:00, 25.08it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:02<00:00, 16.68it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 13.84it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 15.67it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 14.39it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 15.70it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 13.79it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 15.04it/s]\n",
      "Loss: 2.29: 100%|██████████| 50/50 [00:03<00:00, 14.45it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 15.25it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 15.11it/s]\n",
      "Loss: 2.31: 100%|██████████| 50/50 [00:03<00:00, 16.01it/s]\n",
      "Loss: 2.27: 100%|██████████| 50/50 [00:03<00:00, 14.29it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 15.49it/s]\n",
      "Loss: 2.31: 100%|██████████| 50/50 [00:03<00:00, 15.50it/s]\n",
      "Loss: 2.31: 100%|██████████| 50/50 [00:03<00:00, 14.83it/s]\n",
      "Loss: 2.30: 100%|██████████| 50/50 [00:03<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.01, Batch Size:20\n",
      "Score: 0.06\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "model = Model(lr)\n",
    "\n",
    "model.steps = [\n",
    "    Convolution(size=(1, channel_numbers, 4, 4), stride=2),\n",
    "    Relu(),\n",
    "    Flatten(),\n",
    "    FullyConnected(225, 128),\n",
    "    Relu(),\n",
    "    FullyConnected(128, 10)\n",
    "]\n",
    "\n",
    "train(train_data_T, model, batch_size=20, num_epochs=20, lr=lr)\n",
    "predict(model, test_x_T, test_y_T)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.26: 100%|██████████| 32/32 [00:00<00:00, 142.48it/s]\n",
      "Loss: 2.01: 100%|██████████| 32/32 [00:00<00:00, 119.57it/s]\n",
      "Loss: 2.20: 100%|██████████| 32/32 [00:00<00:00, 137.44it/s]\n",
      "Loss: 1.94: 100%|██████████| 32/32 [00:00<00:00, 125.96it/s]\n",
      "Loss: 2.27: 100%|██████████| 32/32 [00:00<00:00, 132.08it/s]\n",
      "Loss: 2.18: 100%|██████████| 32/32 [00:00<00:00, 70.34it/s]\n",
      "Loss: 1.74: 100%|██████████| 32/32 [00:00<00:00, 117.31it/s]\n",
      "Loss: 2.00: 100%|██████████| 32/32 [00:00<00:00, 95.20it/s] \n",
      "Loss: 1.44: 100%|██████████| 32/32 [00:00<00:00, 106.37it/s]\n",
      "Loss: 2.09: 100%|██████████| 32/32 [00:00<00:00, 112.39it/s]\n",
      "Loss: 1.79: 100%|██████████| 32/32 [00:00<00:00, 103.52it/s]\n",
      "Loss: 1.77: 100%|██████████| 32/32 [00:00<00:00, 112.42it/s]\n",
      "Loss: 2.36: 100%|██████████| 32/32 [00:00<00:00, 120.37it/s]\n",
      "Loss: 2.28: 100%|██████████| 32/32 [00:00<00:00, 113.64it/s]\n",
      "Loss: 1.63: 100%|██████████| 32/32 [00:00<00:00, 115.47it/s]\n",
      "Loss: 1.42: 100%|██████████| 32/32 [00:00<00:00, 89.57it/s] \n",
      "Loss: 1.49: 100%|██████████| 32/32 [00:00<00:00, 70.38it/s]\n",
      "Loss: 1.14: 100%|██████████| 32/32 [00:00<00:00, 80.06it/s]\n",
      "Loss: 1.65: 100%|██████████| 32/32 [00:00<00:00, 77.61it/s]\n",
      "Loss: 2.02: 100%|██████████| 32/32 [00:00<00:00, 69.82it/s]\n",
      "Loss: 1.63: 100%|██████████| 32/32 [00:00<00:00, 107.96it/s]\n",
      "Loss: 1.68: 100%|██████████| 32/32 [00:00<00:00, 59.05it/s]\n",
      "Loss: 1.69: 100%|██████████| 32/32 [00:00<00:00, 103.14it/s]\n",
      "Loss: 1.75: 100%|██████████| 32/32 [00:00<00:00, 72.36it/s]\n",
      "Loss: 1.30: 100%|██████████| 32/32 [00:00<00:00, 55.91it/s]\n",
      "Loss: 1.51: 100%|██████████| 32/32 [00:00<00:00, 67.04it/s]\n",
      "Loss: 1.24: 100%|██████████| 32/32 [00:00<00:00, 63.28it/s]\n",
      "Loss: 2.39: 100%|██████████| 32/32 [00:00<00:00, 75.30it/s]\n",
      "Loss: 1.09: 100%|██████████| 32/32 [00:00<00:00, 61.10it/s]\n",
      "Loss: 1.59: 100%|██████████| 32/32 [00:00<00:00, 66.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.1, Batch Size:32\n",
      "Score: 0.24\n"
     ]
    }
   ],
   "source": [
    "lr=0.1\n",
    "model = Model(lr)\n",
    "\n",
    "model.steps = [\n",
    "    Flatten(),\n",
    "    FullyConnected(3072, 100),\n",
    "    Relu(),\n",
    "    FullyConnected(100, 200),\n",
    "    Relu(),\n",
    "    FullyConnected(200, 100),\n",
    "    Relu(),\n",
    "    FullyConnected(100, 10)\n",
    "]\n",
    "\n",
    "train(train_data_T, model, batch_size=32, num_epochs=30, lr=lr)\n",
    "predict(model, test_x_T, test_y_T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.18: 100%|██████████| 32/32 [00:13<00:00,  2.43it/s]\n",
      "Loss: 2.11: 100%|██████████| 32/32 [00:13<00:00,  2.39it/s]\n",
      "Loss: 2.16: 100%|██████████| 32/32 [00:13<00:00,  2.43it/s]\n",
      "Loss: 2.43: 100%|██████████| 32/32 [00:13<00:00,  2.42it/s]\n",
      "Loss: 2.70: 100%|██████████| 32/32 [00:13<00:00,  2.37it/s]\n",
      "Loss: 2.09: 100%|██████████| 32/32 [00:13<00:00,  2.36it/s]\n",
      "Loss: 3.08: 100%|██████████| 32/32 [00:13<00:00,  2.41it/s]\n",
      "Loss: 2.32: 100%|██████████| 32/32 [00:13<00:00,  2.39it/s]\n",
      "Loss: 2.23: 100%|██████████| 32/32 [00:13<00:00,  2.40it/s]\n",
      "Loss: 2.18: 100%|██████████| 32/32 [00:13<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.1, Batch Size:32\n",
      "Score: 0.12\n"
     ]
    }
   ],
   "source": [
    "lr=0.1\n",
    "model = Model(lr)\n",
    "\n",
    "model.steps = [\n",
    "    Convolution(size=(16, 3, 5, 5)),\n",
    "    Relu(),\n",
    "    Flatten(),\n",
    "    FullyConnected(12544, 1000),\n",
    "    Relu(),\n",
    "    FullyConnected(1000, 10)\n",
    "]\n",
    "\n",
    "train(train_data_T, model, batch_size=32, num_epochs=10, lr=lr)\n",
    "predict(model, test_x_T, test_y_T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results\n",
    "### The best NN architecture\n",
    "1. Fully connected (out=100, ReLU)\n",
    "2. Fully connected (out=200, ReLU)\n",
    "3. Fully connected (out=100, ReLU)\n",
    "4. Fully connected (out=10, ReLU)\n",
    "5. Softmax\n",
    "\n",
    "### Learning rate strategy and batch size\n",
    "lr: 0.1\n",
    "\n",
    "batch size: 5000\n",
    "\n",
    "epoch: 150\n",
    "\n",
    "### Data augmentations\n",
    "-\n",
    "\n",
    "### The best model test accuracy\n",
    "0.454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.1, Batch Size:5000\n",
      "Score: 0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.29: 100%|██████████| 10/10 [00:02<00:00,  4.12it/s]\n",
      "Loss: 2.27: 100%|██████████| 10/10 [00:02<00:00,  3.83it/s]\n",
      "Loss: 2.25: 100%|██████████| 10/10 [00:02<00:00,  4.24it/s]\n",
      "Loss: 2.21: 100%|██████████| 10/10 [00:02<00:00,  3.57it/s]\n",
      "Loss: 2.20: 100%|██████████| 10/10 [00:04<00:00,  2.33it/s]\n",
      "Loss: 2.19: 100%|██████████| 10/10 [00:04<00:00,  2.41it/s]\n",
      "Loss: 2.16: 100%|██████████| 10/10 [00:03<00:00,  2.64it/s]\n",
      "Loss: 2.15: 100%|██████████| 10/10 [00:03<00:00,  2.81it/s]\n",
      "Loss: 2.11: 100%|██████████| 10/10 [00:04<00:00,  2.42it/s]\n",
      "Loss: 2.09: 100%|██████████| 10/10 [00:04<00:00,  2.16it/s]\n",
      "Loss: 2.06: 100%|██████████| 10/10 [00:04<00:00,  2.31it/s]\n",
      "Loss: 2.03: 100%|██████████| 10/10 [00:03<00:00,  2.55it/s]\n",
      "Loss: 2.03: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n",
      "Loss: 2.02: 100%|██████████| 10/10 [00:03<00:00,  2.67it/s]\n",
      "Loss: 1.99: 100%|██████████| 10/10 [00:03<00:00,  2.70it/s]\n",
      "Loss: 1.96: 100%|██████████| 10/10 [00:03<00:00,  2.67it/s]\n",
      "Loss: 2.01: 100%|██████████| 10/10 [00:04<00:00,  2.41it/s]\n",
      "Loss: 1.96: 100%|██████████| 10/10 [00:03<00:00,  2.60it/s]\n",
      "Loss: 1.93: 100%|██████████| 10/10 [00:04<00:00,  2.41it/s]\n",
      "Loss: 1.94: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Loss: 1.95: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n",
      "Loss: 1.98: 100%|██████████| 10/10 [00:03<00:00,  2.64it/s]\n",
      "Loss: 1.96: 100%|██████████| 10/10 [00:03<00:00,  2.65it/s]\n",
      "Loss: 1.89: 100%|██████████| 10/10 [00:03<00:00,  2.54it/s]\n",
      "Loss: 1.90: 100%|██████████| 10/10 [00:04<00:00,  2.41it/s]\n",
      "Loss: 1.91: 100%|██████████| 10/10 [00:03<00:00,  2.70it/s]\n",
      "Loss: 1.86: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Loss: 1.84: 100%|██████████| 10/10 [00:03<00:00,  2.64it/s]\n",
      "Loss: 1.90: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n",
      "Loss: 1.93: 100%|██████████| 10/10 [00:04<00:00,  2.40it/s]\n",
      "Loss: 1.84: 100%|██████████| 10/10 [00:05<00:00,  1.82it/s]\n",
      "Loss: 1.84: 100%|██████████| 10/10 [00:04<00:00,  2.22it/s]\n",
      "Loss: 1.91: 100%|██████████| 10/10 [00:04<00:00,  2.33it/s]\n",
      "Loss: 1.85: 100%|██████████| 10/10 [00:05<00:00,  1.95it/s]\n",
      "Loss: 1.81: 100%|██████████| 10/10 [00:03<00:00,  2.60it/s]\n",
      "Loss: 1.84: 100%|██████████| 10/10 [00:04<00:00,  2.17it/s]\n",
      "Loss: 1.88: 100%|██████████| 10/10 [00:03<00:00,  2.61it/s]\n",
      "Loss: 1.94: 100%|██████████| 10/10 [00:03<00:00,  2.64it/s]\n",
      "Loss: 1.87: 100%|██████████| 10/10 [00:04<00:00,  2.37it/s]\n",
      "Loss: 1.82: 100%|██████████| 10/10 [00:04<00:00,  2.30it/s]\n",
      "Loss: 1.88: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n",
      "Loss: 1.85: 100%|██████████| 10/10 [00:04<00:00,  2.28it/s]\n",
      "Loss: 1.82: 100%|██████████| 10/10 [00:03<00:00,  2.55it/s]\n",
      "Loss: 1.81: 100%|██████████| 10/10 [00:03<00:00,  2.59it/s]\n",
      "Loss: 1.78: 100%|██████████| 10/10 [00:03<00:00,  2.55it/s]\n",
      "Loss: 1.81: 100%|██████████| 10/10 [00:03<00:00,  2.54it/s]\n",
      "Loss: 1.78: 100%|██████████| 10/10 [00:03<00:00,  2.87it/s]\n",
      "Loss: 1.77: 100%|██████████| 10/10 [00:03<00:00,  3.00it/s]\n",
      "Loss: 1.74: 100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n",
      "Loss: 1.83: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]\n",
      "Loss: 1.81: 100%|██████████| 10/10 [00:04<00:00,  2.14it/s]\n",
      "Loss: 1.82: 100%|██████████| 10/10 [00:04<00:00,  2.15it/s]\n",
      "Loss: 1.73: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n",
      "Loss: 1.73: 100%|██████████| 10/10 [00:03<00:00,  2.51it/s]\n",
      "Loss: 1.76: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n",
      "Loss: 1.71: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n",
      "Loss: 1.79: 100%|██████████| 10/10 [00:04<00:00,  2.41it/s]\n",
      "Loss: 1.71: 100%|██████████| 10/10 [00:03<00:00,  2.61it/s]\n",
      "Loss: 1.80: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s]\n",
      "Loss: 1.76: 100%|██████████| 10/10 [00:04<00:00,  2.41it/s]\n",
      "Loss: 1.74: 100%|██████████| 10/10 [00:04<00:00,  2.46it/s]\n",
      "Loss: 1.78: 100%|██████████| 10/10 [00:03<00:00,  2.55it/s]\n",
      "Loss: 1.77: 100%|██████████| 10/10 [00:03<00:00,  2.51it/s]\n",
      "Loss: 1.82: 100%|██████████| 10/10 [00:04<00:00,  2.46it/s]\n",
      "Loss: 1.72: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n",
      "Loss: 1.72: 100%|██████████| 10/10 [00:04<00:00,  2.42it/s]\n",
      "Loss: 1.73: 100%|██████████| 10/10 [00:04<00:00,  2.15it/s]\n",
      "Loss: 1.78: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s]\n",
      "Loss: 1.75: 100%|██████████| 10/10 [00:03<00:00,  2.54it/s]\n",
      "Loss: 1.71: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Loss: 1.73: 100%|██████████| 10/10 [00:03<00:00,  2.50it/s]\n",
      "Loss: 1.69: 100%|██████████| 10/10 [00:03<00:00,  2.63it/s]\n",
      "Loss: 1.73: 100%|██████████| 10/10 [00:05<00:00,  1.91it/s]\n",
      "Loss: 1.70: 100%|██████████| 10/10 [00:05<00:00,  1.74it/s]\n",
      "Loss: 1.68: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Loss: 1.71: 100%|██████████| 10/10 [00:04<00:00,  2.11it/s]\n",
      "Loss: 1.74: 100%|██████████| 10/10 [00:04<00:00,  2.00it/s]\n",
      "Loss: 1.67: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n",
      "Loss: 1.72: 100%|██████████| 10/10 [00:04<00:00,  2.12it/s]\n",
      "Loss: 1.72: 100%|██████████| 10/10 [00:04<00:00,  2.12it/s]\n",
      "Loss: 1.66: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s]\n",
      "Loss: 1.68: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s]\n",
      "Loss: 1.72: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Loss: 1.67: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Loss: 1.69: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n",
      "Loss: 1.62: 100%|██████████| 10/10 [00:04<00:00,  2.38it/s]\n",
      "Loss: 1.66: 100%|██████████| 10/10 [00:03<00:00,  2.54it/s]\n",
      "Loss: 1.64: 100%|██████████| 10/10 [00:04<00:00,  2.45it/s]\n",
      "Loss: 1.69: 100%|██████████| 10/10 [00:03<00:00,  2.52it/s]\n",
      "Loss: 1.68: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]\n",
      "Loss: 1.71: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]\n",
      "Loss: 1.66: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n",
      "Loss: 1.64: 100%|██████████| 10/10 [00:04<00:00,  2.45it/s]\n",
      "Loss: 1.62: 100%|██████████| 10/10 [00:03<00:00,  2.51it/s]\n",
      "Loss: 1.67: 100%|██████████| 10/10 [00:03<00:00,  2.53it/s]\n",
      "Loss: 1.61: 100%|██████████| 10/10 [00:03<00:00,  2.51it/s]\n",
      "Loss: 1.79: 100%|██████████| 10/10 [00:03<00:00,  2.62it/s]\n",
      "Loss: 1.66: 100%|██████████| 10/10 [00:04<00:00,  2.10it/s]\n",
      "Loss: 1.69: 100%|██████████| 10/10 [00:03<00:00,  2.52it/s]\n",
      "Loss: 1.61: 100%|██████████| 10/10 [00:04<00:00,  2.22it/s]\n",
      "Loss: 1.64: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n",
      "Loss: 1.60: 100%|██████████| 10/10 [00:04<00:00,  2.23it/s]\n",
      "Loss: 1.63: 100%|██████████| 10/10 [00:03<00:00,  2.58it/s]\n",
      "Loss: 1.63: 100%|██████████| 10/10 [00:04<00:00,  2.38it/s]\n",
      "Loss: 1.58: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]\n",
      "Loss: 1.69: 100%|██████████| 10/10 [00:03<00:00,  2.84it/s]\n",
      "Loss: 1.63: 100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n",
      "Loss: 1.62: 100%|██████████| 10/10 [00:03<00:00,  3.00it/s]\n",
      "Loss: 1.61: 100%|██████████| 10/10 [00:03<00:00,  3.19it/s]\n",
      "Loss: 1.58: 100%|██████████| 10/10 [00:03<00:00,  3.10it/s]\n",
      "Loss: 1.58: 100%|██████████| 10/10 [00:04<00:00,  2.22it/s]\n",
      "Loss: 1.63: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Loss: 1.63: 100%|██████████| 10/10 [00:04<00:00,  2.37it/s]\n",
      "Loss: 1.57: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n",
      "Loss: 1.66: 100%|██████████| 10/10 [00:04<00:00,  2.16it/s]\n",
      "Loss: 1.64: 100%|██████████| 10/10 [00:03<00:00,  2.52it/s]\n",
      "Loss: 1.58: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Loss: 1.59: 100%|██████████| 10/10 [00:03<00:00,  2.53it/s]\n",
      "Loss: 1.59: 100%|██████████| 10/10 [00:04<00:00,  2.44it/s]\n",
      "Loss: 1.57: 100%|██████████| 10/10 [00:03<00:00,  2.55it/s]\n",
      "Loss: 1.59: 100%|██████████| 10/10 [00:04<00:00,  2.36it/s]\n",
      "Loss: 1.61: 100%|██████████| 10/10 [00:03<00:00,  2.56it/s]\n",
      "Loss: 1.54: 100%|██████████| 10/10 [00:04<00:00,  2.44it/s]\n",
      "Loss: 1.61: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n",
      "Loss: 1.56: 100%|██████████| 10/10 [00:04<00:00,  2.44it/s]\n",
      "Loss: 1.58: 100%|██████████| 10/10 [00:04<00:00,  2.33it/s]\n",
      "Loss: 1.64: 100%|██████████| 10/10 [00:04<00:00,  2.40it/s]\n",
      "Loss: 1.57: 100%|██████████| 10/10 [00:04<00:00,  2.36it/s]\n",
      "Loss: 1.58: 100%|██████████| 10/10 [00:04<00:00,  2.31it/s]\n",
      "Loss: 1.61: 100%|██████████| 10/10 [00:04<00:00,  2.17it/s]\n",
      "Loss: 1.55: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]\n",
      "Loss: 1.59: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]\n",
      "Loss: 1.59: 100%|██████████| 10/10 [00:04<00:00,  2.41it/s]\n",
      "Loss: 1.56: 100%|██████████| 10/10 [00:03<00:00,  2.62it/s]\n",
      "Loss: 1.62: 100%|██████████| 10/10 [00:03<00:00,  2.56it/s]\n",
      "Loss: 1.67: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Loss: 1.55: 100%|██████████| 10/10 [00:04<00:00,  2.25it/s]\n",
      "Loss: 1.58: 100%|██████████| 10/10 [00:04<00:00,  2.26it/s]\n",
      "Loss: 1.57: 100%|██████████| 10/10 [00:03<00:00,  2.52it/s]\n",
      "Loss: 1.54: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]\n",
      "Loss: 1.56: 100%|██████████| 10/10 [00:04<00:00,  2.37it/s]\n",
      "Loss: 1.55: 100%|██████████| 10/10 [00:04<00:00,  2.19it/s]\n",
      "Loss: 1.61: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n",
      "Loss: 1.58: 100%|██████████| 10/10 [00:04<00:00,  2.38it/s]\n",
      "Loss: 1.51: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n",
      "Loss: 1.61: 100%|██████████| 10/10 [00:04<00:00,  2.38it/s]\n",
      "Loss: 1.51: 100%|██████████| 10/10 [00:04<00:00,  2.17it/s]\n",
      "Loss: 1.58: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]\n",
      "Loss: 1.54: 100%|██████████| 10/10 [00:04<00:00,  2.44it/s]\n",
      "Loss: 1.55: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "lr=0.1\n",
    "model = Model(lr)\n",
    "\n",
    "model.steps = [\n",
    "    Flatten(),\n",
    "    FullyConnected(3072, 100),\n",
    "    Relu(),\n",
    "    FullyConnected(100, 200),\n",
    "    Relu(),\n",
    "    FullyConnected(200, 100),\n",
    "    Relu(),\n",
    "    FullyConnected(100, 10)\n",
    "]\n",
    "\n",
    "train(train_data, model, batch_size=5000, num_epochs=150, lr=lr)\n",
    "predict(model, test_x, test_y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}